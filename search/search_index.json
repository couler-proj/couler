{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Couler? \u00b6 Couler aims to provide a unified interface for constructing and managing workflows on different workflow engines, such as Argo Workflows , Tekton Pipelines , and Apache Airflow . Couler is included in CNCF Cloud Native Landscape and LF AI Landscape . Who uses Couler? \u00b6 You can find a list of organizations who are using Couler in Adopters . If you'd like to add your organization to the list, please send us a pull request. Why use Couler? \u00b6 Many workflow engines exist nowadays, e.g. Argo Workflows , Tekton Pipelines , and Apache Airflow . However, their programming experience varies and they have different level of abstractions that are often obscure and complex. The code snippets below are some examples for constructing workflows using Apache Airflow and Kubeflow Pipelines . Apache Airflow def create_dag ( dag_id , schedule , dag_number , default_args ): def hello_world_py ( * args ): print ( 'Hello World' ) dag = DAG ( dag_id , schedule_interval = schedule , default_args = default_args ) with dag : t1 = PythonOperator ( task_id = 'hello_world' , python_callable = hello_world_py , dag_number = dag_number ) return dag for n in range ( 1 , 10 ): default_args = { 'owner' : 'airflow' , 'start_date' : datetime ( 2018 , 1 , 1 ) } globals ()[ dag_id ] = create_dag ( 'hello_world_ {} ' . format ( str ( n )), '@daily' , n , default_args ) Kubeflow Pipelines class FlipCoinOp ( dsl . ContainerOp ): \"\"\"Flip a coin and output heads or tails randomly.\"\"\" def __init__ ( self ): super ( FlipCoinOp , self ) . __init__ ( name = 'Flip' , image = 'python:alpine3.6' , command = [ 'sh' , '-c' ], arguments = [ 'python -c \"import random; result = \\' heads \\' if random.randint(0,1) == 0 ' 'else \\' tails \\' ; print(result)\" | tee /tmp/output' ], file_outputs = { 'output' : '/tmp/output' }) class PrintOp ( dsl . ContainerOp ): \"\"\"Print a message.\"\"\" def __init__ ( self , msg ): super ( PrintOp , self ) . __init__ ( name = 'Print' , image = 'alpine:3.6' , command = [ 'echo' , msg ], ) # define the recursive operation @graph_component def flip_component ( flip_result ): print_flip = PrintOp ( flip_result ) flipA = FlipCoinOp () . after ( print_flip ) with dsl . Condition ( flipA . output == 'heads' ): flip_component ( flipA . output ) @dsl . pipeline ( name = 'pipeline flip coin' , description = 'shows how to use graph_component.' ) def recursive (): flipA = FlipCoinOp () flipB = FlipCoinOp () flip_loop = flip_component ( flipA . output ) flip_loop . after ( flipB ) PrintOp ( 'cool, it is over. %s ' % flipA . output ) . after ( flip_loop ) Couler provides a unified interface for constructing and managing workflows that provides the following: Simplicity: Unified interface and imperative programming style for defining workflows with automatic construction of directed acyclic graph (DAG). Extensibility: Extensible to support various workflow engines. Reusability: Reusable steps for tasks such as distributed training of machine learning models. Efficiency: Automatic workflow and resource optimizations under the hood. Please see the following sections for installation guide and examples. Documentation \u00b6 Getting Started \u00b6 To set up Couler and run your first workflow, please see Getting Started . Examples \u00b6 For more examples of Couler usage, please see Examples .","title":"Overview"},{"location":"#what-is-couler","text":"Couler aims to provide a unified interface for constructing and managing workflows on different workflow engines, such as Argo Workflows , Tekton Pipelines , and Apache Airflow . Couler is included in CNCF Cloud Native Landscape and LF AI Landscape .","title":"What is Couler?"},{"location":"#who-uses-couler","text":"You can find a list of organizations who are using Couler in Adopters . If you'd like to add your organization to the list, please send us a pull request.","title":"Who uses Couler?"},{"location":"#why-use-couler","text":"Many workflow engines exist nowadays, e.g. Argo Workflows , Tekton Pipelines , and Apache Airflow . However, their programming experience varies and they have different level of abstractions that are often obscure and complex. The code snippets below are some examples for constructing workflows using Apache Airflow and Kubeflow Pipelines . Apache Airflow def create_dag ( dag_id , schedule , dag_number , default_args ): def hello_world_py ( * args ): print ( 'Hello World' ) dag = DAG ( dag_id , schedule_interval = schedule , default_args = default_args ) with dag : t1 = PythonOperator ( task_id = 'hello_world' , python_callable = hello_world_py , dag_number = dag_number ) return dag for n in range ( 1 , 10 ): default_args = { 'owner' : 'airflow' , 'start_date' : datetime ( 2018 , 1 , 1 ) } globals ()[ dag_id ] = create_dag ( 'hello_world_ {} ' . format ( str ( n )), '@daily' , n , default_args ) Kubeflow Pipelines class FlipCoinOp ( dsl . ContainerOp ): \"\"\"Flip a coin and output heads or tails randomly.\"\"\" def __init__ ( self ): super ( FlipCoinOp , self ) . __init__ ( name = 'Flip' , image = 'python:alpine3.6' , command = [ 'sh' , '-c' ], arguments = [ 'python -c \"import random; result = \\' heads \\' if random.randint(0,1) == 0 ' 'else \\' tails \\' ; print(result)\" | tee /tmp/output' ], file_outputs = { 'output' : '/tmp/output' }) class PrintOp ( dsl . ContainerOp ): \"\"\"Print a message.\"\"\" def __init__ ( self , msg ): super ( PrintOp , self ) . __init__ ( name = 'Print' , image = 'alpine:3.6' , command = [ 'echo' , msg ], ) # define the recursive operation @graph_component def flip_component ( flip_result ): print_flip = PrintOp ( flip_result ) flipA = FlipCoinOp () . after ( print_flip ) with dsl . Condition ( flipA . output == 'heads' ): flip_component ( flipA . output ) @dsl . pipeline ( name = 'pipeline flip coin' , description = 'shows how to use graph_component.' ) def recursive (): flipA = FlipCoinOp () flipB = FlipCoinOp () flip_loop = flip_component ( flipA . output ) flip_loop . after ( flipB ) PrintOp ( 'cool, it is over. %s ' % flipA . output ) . after ( flip_loop ) Couler provides a unified interface for constructing and managing workflows that provides the following: Simplicity: Unified interface and imperative programming style for defining workflows with automatic construction of directed acyclic graph (DAG). Extensibility: Extensible to support various workflow engines. Reusability: Reusable steps for tasks such as distributed training of machine learning models. Efficiency: Automatic workflow and resource optimizations under the hood. Please see the following sections for installation guide and examples.","title":"Why use Couler?"},{"location":"#documentation","text":"","title":"Documentation"},{"location":"#getting-started","text":"To set up Couler and run your first workflow, please see Getting Started .","title":"Getting Started"},{"location":"#examples","text":"For more examples of Couler usage, please see Examples .","title":"Examples"},{"location":"TEMPLATE/","text":"Note to proposers: Please keep this document as brief as possible, preferably not more than two pages. Motivation \u00b6 A high level description of the problem or opportunity being addressed. Goals \u00b6 Specific new functionalities or other changes. Non-Goals \u00b6 Issues or changes not being addressed by this proposal. Design \u00b6 Description of new software design and any major changes to existing software. Should include figures or diagrams where appropriate. Backward compatibility must be considered. Alternatives Considered \u00b6 Description of possible alternative solutions and the reasons they were not chosen.","title":"Template"},{"location":"TEMPLATE/#motivation","text":"A high level description of the problem or opportunity being addressed.","title":"Motivation"},{"location":"TEMPLATE/#goals","text":"Specific new functionalities or other changes.","title":"Goals"},{"location":"TEMPLATE/#non-goals","text":"Issues or changes not being addressed by this proposal.","title":"Non-Goals"},{"location":"TEMPLATE/#design","text":"Description of new software design and any major changes to existing software. Should include figures or diagrams where appropriate. Backward compatibility must be considered.","title":"Design"},{"location":"TEMPLATE/#alternatives-considered","text":"Description of possible alternative solutions and the reasons they were not chosen.","title":"Alternatives Considered"},{"location":"adopters/","text":"Adopters of Couler \u00b6 This page contains a list of organizations who are using Couler. If you'd like to be included here, please send a pull request which modifies this file. Ant Group Bytedance Determined FreeWheel Konnecto Onepanel Pipekit SQLFlow","title":"Adopters"},{"location":"adopters/#adopters-of-couler","text":"This page contains a list of organizations who are using Couler. If you'd like to be included here, please send a pull request which modifies this file. Ant Group Bytedance Determined FreeWheel Konnecto Onepanel Pipekit SQLFlow","title":"Adopters of Couler"},{"location":"contributing/","text":"# Contributing Guide Welcome to Couler's contributing guide! Install Dependencies \u00b6 You can install all the dependent Python packages for development via the following: python -m pip install --upgrade pip python -m pip install -r requirements.txt -r requirements-dev.txt Run Unit Tests \u00b6 You can execute all the unit tests via the following command: python setup.py install python -m pytest Run Integration Tests \u00b6 The current integration test suite requires: kubectl minikube Star a k8s cluster using minikube: minikube config set vm-driver docker minikube config set kubernetes-version 1 .18.3 minikube start Install Argo Workflows: kubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.11.1/manifests/quick-start-minimal.yaml Run the integration tests: scripts/integration_tests.sh Run Sanity Checks \u00b6 We use pre-commit to check issues on code style and quality. For example, it runs black for automatic Python code formatting which should fix most of the issues automatically. You can execute the following command to run all the sanity checks: pre-commit run --all Run the Documentation Server \u00b6 If you have modified the documentation, you may want to run the documentation server locally to preview the changes. Couler uses Material for MkDocs to build the documentation. You can run the following command to start a local documentation server: mkdocs serve This will start the documentation server on the port 8000 by default. Sign the Contributor License Agreement (CLA) \u00b6 If you haven't signed the CLA yet, @CLAassistant will notify you on your pull request. Then you can simply follow the provided instructions on the pull request and sign the CLA using your GitHub account. For your convenience, the content of the CLA can be found here .","title":"Contributing"},{"location":"contributing/#install-dependencies","text":"You can install all the dependent Python packages for development via the following: python -m pip install --upgrade pip python -m pip install -r requirements.txt -r requirements-dev.txt","title":"Install Dependencies"},{"location":"contributing/#run-unit-tests","text":"You can execute all the unit tests via the following command: python setup.py install python -m pytest","title":"Run Unit Tests"},{"location":"contributing/#run-integration-tests","text":"The current integration test suite requires: kubectl minikube Star a k8s cluster using minikube: minikube config set vm-driver docker minikube config set kubernetes-version 1 .18.3 minikube start Install Argo Workflows: kubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.11.1/manifests/quick-start-minimal.yaml Run the integration tests: scripts/integration_tests.sh","title":"Run Integration Tests"},{"location":"contributing/#run-sanity-checks","text":"We use pre-commit to check issues on code style and quality. For example, it runs black for automatic Python code formatting which should fix most of the issues automatically. You can execute the following command to run all the sanity checks: pre-commit run --all","title":"Run Sanity Checks"},{"location":"contributing/#run-the-documentation-server","text":"If you have modified the documentation, you may want to run the documentation server locally to preview the changes. Couler uses Material for MkDocs to build the documentation. You can run the following command to start a local documentation server: mkdocs serve This will start the documentation server on the port 8000 by default.","title":"Run the Documentation Server"},{"location":"contributing/#sign-the-contributor-license-agreement-cla","text":"If you haven't signed the CLA yet, @CLAassistant will notify you on your pull request. Then you can simply follow the provided instructions on the pull request and sign the CLA using your GitHub account. For your convenience, the content of the CLA can be found here .","title":"Sign the Contributor License Agreement (CLA)"},{"location":"couler-api-design/","text":"Couler API Design \u00b6 This document outlines the design of core Couler APIs to support multiple workflow backends. Please see this RFC for the initial design discussions. Goals \u00b6 Design the core Couler APIs that can be implemented across different workflow engines. Provide a minimal set of APIs that are forward-looking, engine-agnostic, and less likely to deprecate over-time. Non-Goals \u00b6 Provide an exhaustive set of APIs that cover all use cases and all workflow engines. Provide implementation details. Design \u00b6 Core operations ( couler.ops ): run_step(step_def) where step_def is the step definition that can be a container spec, Python function, or spec that's specific to the underlying workflow engine (e.g. k8s CRD if Argo Workflow is used). A \"step\" represents a node in the workflow graph, e.g. the \"smallest\" unit in some sense. For an analogy of a Couler \"step\" in different backends, please see the last section for a table of various Couler concepts. Control flow ( couler.control_flows ): map(func, *args, **kwargs) where *args and **kwargs correspond to various arguments passed to the function func . when(cond, if_op, else_op) where cond can be any of the following predicates: equal() , not_equal() , bigger() , smaller() , bigger_equal() , smaller_equal() . The operation defined in if_op will be executed when cond is true. Otherwise, else_op will be executed. while_loop(cond, func, *args, **kwargs) where cond can be any one of the predicates mentioned above. func , *args , and **kwargs are similar to map() 's. Utilities ( couler.utils ): submit(config=workflow_config(schedule=\"* * * * 1\")) where config is engine-specific. get_status(workflow_name) get_logs(workflow_name) delete_workflow(workflow_name) Backends ( couler.backends ): get_backend() use_backend(\"argo\") Minimal Working Workflow Example \u00b6 An example workflow defined via some of the APIs mentioned above is shown below: import couler import random if couler . get_backend () != \"argo\" : couler . use_backend ( \"argo\" ) def random_code (): result = \"heads\" if random . randint ( 0 , 1 ) == 0 else \"tails\" print ( result ) def flip_coin (): return couler . run_step ( image = \"python:alpine3.6\" , step_def = random_code , ) def heads (): return couler . run_step ( image = \"alpine:3.6\" , step_def = [ \"bash\" , \"-c\" , 'echo \"it was heads\"' ], ) def tails (): return couler . run_step ( image = \"alpine:3.6\" , step_def = [ \"bash\" , \"-c\" , 'echo \"it was tails\"' ], ) result = flip_coin () couler . when ( couler . equal ( result , \"heads\" ), lambda : heads ()) couler . when ( couler . equal ( result , \"tails\" ), lambda : tails ()) name = couler . submit ( config = workflow_config ( schedule = \"* * * * 1\" )) while couler . get_status ( name ) == \"Running\" : if couler . get_status ( name ) == \"Completed\" : couler . delete_workflow ( name ) break Concepts Analogies in Different Backends \u00b6 To help visualize various concepts and their analogies in different Backends, below is an attempt to compare them in a table which will be updated over time as support for new backends is proposed: Concept\\Framework Couler Argo Tekton Airflow Dagster Prefect Step Step Step Step Task Solid Task Composite step Reusable step Template Task SubDag or TaskGroup Composite Solid TBA Worfklow Workflow Workflow Pipeline DAG Pipeline Flow Note that by \"reusable step\", we mean parameterized templates that can be used to define a Couler step where users only have to specify a few parameters.","title":"API Design"},{"location":"couler-api-design/#couler-api-design","text":"This document outlines the design of core Couler APIs to support multiple workflow backends. Please see this RFC for the initial design discussions.","title":"Couler API Design"},{"location":"couler-api-design/#goals","text":"Design the core Couler APIs that can be implemented across different workflow engines. Provide a minimal set of APIs that are forward-looking, engine-agnostic, and less likely to deprecate over-time.","title":"Goals"},{"location":"couler-api-design/#non-goals","text":"Provide an exhaustive set of APIs that cover all use cases and all workflow engines. Provide implementation details.","title":"Non-Goals"},{"location":"couler-api-design/#design","text":"Core operations ( couler.ops ): run_step(step_def) where step_def is the step definition that can be a container spec, Python function, or spec that's specific to the underlying workflow engine (e.g. k8s CRD if Argo Workflow is used). A \"step\" represents a node in the workflow graph, e.g. the \"smallest\" unit in some sense. For an analogy of a Couler \"step\" in different backends, please see the last section for a table of various Couler concepts. Control flow ( couler.control_flows ): map(func, *args, **kwargs) where *args and **kwargs correspond to various arguments passed to the function func . when(cond, if_op, else_op) where cond can be any of the following predicates: equal() , not_equal() , bigger() , smaller() , bigger_equal() , smaller_equal() . The operation defined in if_op will be executed when cond is true. Otherwise, else_op will be executed. while_loop(cond, func, *args, **kwargs) where cond can be any one of the predicates mentioned above. func , *args , and **kwargs are similar to map() 's. Utilities ( couler.utils ): submit(config=workflow_config(schedule=\"* * * * 1\")) where config is engine-specific. get_status(workflow_name) get_logs(workflow_name) delete_workflow(workflow_name) Backends ( couler.backends ): get_backend() use_backend(\"argo\")","title":"Design"},{"location":"couler-api-design/#minimal-working-workflow-example","text":"An example workflow defined via some of the APIs mentioned above is shown below: import couler import random if couler . get_backend () != \"argo\" : couler . use_backend ( \"argo\" ) def random_code (): result = \"heads\" if random . randint ( 0 , 1 ) == 0 else \"tails\" print ( result ) def flip_coin (): return couler . run_step ( image = \"python:alpine3.6\" , step_def = random_code , ) def heads (): return couler . run_step ( image = \"alpine:3.6\" , step_def = [ \"bash\" , \"-c\" , 'echo \"it was heads\"' ], ) def tails (): return couler . run_step ( image = \"alpine:3.6\" , step_def = [ \"bash\" , \"-c\" , 'echo \"it was tails\"' ], ) result = flip_coin () couler . when ( couler . equal ( result , \"heads\" ), lambda : heads ()) couler . when ( couler . equal ( result , \"tails\" ), lambda : tails ()) name = couler . submit ( config = workflow_config ( schedule = \"* * * * 1\" )) while couler . get_status ( name ) == \"Running\" : if couler . get_status ( name ) == \"Completed\" : couler . delete_workflow ( name ) break","title":"Minimal Working Workflow Example"},{"location":"couler-api-design/#concepts-analogies-in-different-backends","text":"To help visualize various concepts and their analogies in different Backends, below is an attempt to compare them in a table which will be updated over time as support for new backends is proposed: Concept\\Framework Couler Argo Tekton Airflow Dagster Prefect Step Step Step Step Task Solid Task Composite step Reusable step Template Task SubDag or TaskGroup Composite Solid TBA Worfklow Workflow Workflow Pipeline DAG Pipeline Flow Note that by \"reusable step\", we mean parameterized templates that can be used to define a Couler step where users only have to specify a few parameters.","title":"Concepts Analogies in Different Backends"},{"location":"couler-step-zoo/","text":"Couler Step Zoo \u00b6 This document introduces the Couler Step Zoo, which consists of a collection of pre-defined and reusable steps that can be used directly as part of a workflow defined using Couler. Existing Steps \u00b6 Currently, Couler Step Zoo consists of the following pre-defined steps: Name Description API MPI Submit MPI-based distributed jobs via Kubeflow MPI Operator couler.steps.mpi.train() TensorFlow Submit TensorFlow distributed training jobs via Kubeflow TF Operator couler.steps.tensorflow.train() PyTorch Submit PyTorch distributed training jobs via Kubeflow PyTorch Operator couler.steps.pytorch.train() Katib Submit AutoML experiments (e.g. hyperparameter tuning and neural architecture search) via Kubeflow Katib couler.steps.katib.run() If you'd like to add a new pre-defined steps to the Couler Step Zoo, please see the following sections for the specific requirements and instructions. Requirements on New Steps \u00b6 In order to provide a consistent and friendly experience to our users, below are the requirements for a new pre-defined step to be eligible for inclusion in Couler Step Zoo: It should be completely implemented using the core Couler APIs . It should expose backend specific configurations instead of hard-coding them. It should have a clear set of dependencies that can be easily installed with sufficient instructions. When possible, proving a minimal set of unit tests and integration tests to make sure the step functions correctly. Adding a Pre-defined Step \u00b6 To add a pre-defined step to the Couler Step Zoo, please follow the instructions below. Make sure the step meets the list of requirements in the above section. Add the step implementation to couler.steps module . The interface would look like the following: def random_code (): import random res = \"heads\" if random . randint ( 0 , 1 ) == 0 else \"tails\" print ( res ) def run_heads ( image = \"alpine:3.6\" , command = [ \"sh\" , \"-c\" , 'echo \"it was heads\"' ], ): result = random_code () couler . when ( couler . equal ( result , \"heads\" ), lambda : couler . run_step ( command = command , image = image , )) Here we implemented a pre-defined step run_heads() that could run a specified command such as [\"sh\", \"-c\", 'echo \"it was heads\"'] if random_code() returns \"heads\" . Note that the step should be completely implemented using the core Couler APIs in order to work well with different Couler backends. You can find reference step implementations here . Provide minimal set of unit test and integration test when possible. Provide necessary user-facing documentation in the API docstring, including documentation on each arguments in the step signature and the system dependencies. Alternatives Considered \u00b6 Some workflow engines or frameworks provide their own library for distributing the set of reusable steps/tasks/components, for example: Argo Workflows catalog Prefect tasks library KFP components Tekton tasks/pipelines catalog Even though it's relatively easier to provide wrappers around the existing reusable libraries, there are some issues with that approach: It's hard to maintain and keep the wrappers up-to-date. It's non-trivial to provide a consistent interface that would work across different backends. This would introduce bad user experience due to feature parity across those reusable libraries implemented for different backends.","title":"Step Zoo"},{"location":"couler-step-zoo/#couler-step-zoo","text":"This document introduces the Couler Step Zoo, which consists of a collection of pre-defined and reusable steps that can be used directly as part of a workflow defined using Couler.","title":"Couler Step Zoo"},{"location":"couler-step-zoo/#existing-steps","text":"Currently, Couler Step Zoo consists of the following pre-defined steps: Name Description API MPI Submit MPI-based distributed jobs via Kubeflow MPI Operator couler.steps.mpi.train() TensorFlow Submit TensorFlow distributed training jobs via Kubeflow TF Operator couler.steps.tensorflow.train() PyTorch Submit PyTorch distributed training jobs via Kubeflow PyTorch Operator couler.steps.pytorch.train() Katib Submit AutoML experiments (e.g. hyperparameter tuning and neural architecture search) via Kubeflow Katib couler.steps.katib.run() If you'd like to add a new pre-defined steps to the Couler Step Zoo, please see the following sections for the specific requirements and instructions.","title":"Existing Steps"},{"location":"couler-step-zoo/#requirements-on-new-steps","text":"In order to provide a consistent and friendly experience to our users, below are the requirements for a new pre-defined step to be eligible for inclusion in Couler Step Zoo: It should be completely implemented using the core Couler APIs . It should expose backend specific configurations instead of hard-coding them. It should have a clear set of dependencies that can be easily installed with sufficient instructions. When possible, proving a minimal set of unit tests and integration tests to make sure the step functions correctly.","title":"Requirements on New Steps"},{"location":"couler-step-zoo/#adding-a-pre-defined-step","text":"To add a pre-defined step to the Couler Step Zoo, please follow the instructions below. Make sure the step meets the list of requirements in the above section. Add the step implementation to couler.steps module . The interface would look like the following: def random_code (): import random res = \"heads\" if random . randint ( 0 , 1 ) == 0 else \"tails\" print ( res ) def run_heads ( image = \"alpine:3.6\" , command = [ \"sh\" , \"-c\" , 'echo \"it was heads\"' ], ): result = random_code () couler . when ( couler . equal ( result , \"heads\" ), lambda : couler . run_step ( command = command , image = image , )) Here we implemented a pre-defined step run_heads() that could run a specified command such as [\"sh\", \"-c\", 'echo \"it was heads\"'] if random_code() returns \"heads\" . Note that the step should be completely implemented using the core Couler APIs in order to work well with different Couler backends. You can find reference step implementations here . Provide minimal set of unit test and integration test when possible. Provide necessary user-facing documentation in the API docstring, including documentation on each arguments in the step signature and the system dependencies.","title":"Adding a Pre-defined Step"},{"location":"couler-step-zoo/#alternatives-considered","text":"Some workflow engines or frameworks provide their own library for distributing the set of reusable steps/tasks/components, for example: Argo Workflows catalog Prefect tasks library KFP components Tekton tasks/pipelines catalog Even though it's relatively easier to provide wrappers around the existing reusable libraries, there are some issues with that approach: It's hard to maintain and keep the wrappers up-to-date. It's non-trivial to provide a consistent interface that would work across different backends. This would introduce bad user experience due to feature parity across those reusable libraries implemented for different backends.","title":"Alternatives Considered"},{"location":"couler-tekton-design/","text":"Couler Tekton Design \u00b6 This document outlines the design of tekton backend for core Couler APIs. Goals \u00b6 Design the tekton backend implementation for core Couler APIs. Non-Goals \u00b6 Provide implementation details. Design \u00b6 Based on the core Couler APIs design , we can see the definitions in different workflow engines. Couler Argo Tekton Step Step Step Reusable step Template Task Workflow Workflow Pipeline Step is the smallest unit and remain consistent across different backends so the core operation run_step(step_def) will also keep consistent. In tekton, the definition of step and container keeps the same, which means that if users pass a python function to step, the backend should wrap the function in a python base image automatically. Control Flow \u00b6 when(cond, if_op, else_op) \u00b6 In the latest v0.16 version of Tekton, the condition field is deprecated and Tekton uses whenExpression instead. Here is an example: tasks : - name : echo-file-exists when : - input : \"$(tasks.check-file.results.exists)\" operator : in values : [ \"yes\" ] taskRef : name : echo-file-exists A whenExpression is made of Input , Operator and Values . The Input can be static inputs or variables (Parameters or Results in Tekton). The Operator can be either in or notin . As we can see, there is no else_op in Tekton, so we need to convert the else in the Tekton backend, which is pretty simple since there is only in and notin operator, we can simply change the operator in the \"else\" task like: tasks : - name : echo-file-not-exists when : - input : \"$(tasks.check-file.results.exists)\" operator : notin values : [ \"yes\" ] taskRef : name : echo-file-not-exists while_loop(cond, func, *args, **kwargs) \u00b6 There is no native support for loops in Tekton for now. See this issue for more information. Couler Utilities: \u00b6 The get_status , get_logs remains the same as the Argo design. submit(config=workflow_config(schedule=\"* * * * 1\")) : There is no native support for cron in Tekton, see this issue for more information. The community recommend cronjob in kubernetes, see example here. For couler, we can wrap the cron in the workflow and let backend to handle the work like create CronJob in Kubernetes. delete_workflow(workflow_name) : To be discussed: should we delete the tasks when delete the pipeline? list_workflow_records(workflow_name) : After submitting the workflow(create PipelineRun in Tekton), there will be numbers of workflow records. A list function is necessary for users to show all the records. Other Tekton Utilities \u00b6 Finally : Finally is a unique feature of Tekton. With finally, the Final tasks are guaranteed to be executed in parallel after all PipelineTasks under tasks have completed regardless of success or error. For example: spec : tasks : - name : tests taskRef : Name : integration-test finally : - name : cleanup-test taskRef : Name : cleanup In Couler, we can have run_finally(finally_def) function which is only available when the backend is Tekton.","title":"Tekton Design"},{"location":"couler-tekton-design/#couler-tekton-design","text":"This document outlines the design of tekton backend for core Couler APIs.","title":"Couler Tekton Design"},{"location":"couler-tekton-design/#goals","text":"Design the tekton backend implementation for core Couler APIs.","title":"Goals"},{"location":"couler-tekton-design/#non-goals","text":"Provide implementation details.","title":"Non-Goals"},{"location":"couler-tekton-design/#design","text":"Based on the core Couler APIs design , we can see the definitions in different workflow engines. Couler Argo Tekton Step Step Step Reusable step Template Task Workflow Workflow Pipeline Step is the smallest unit and remain consistent across different backends so the core operation run_step(step_def) will also keep consistent. In tekton, the definition of step and container keeps the same, which means that if users pass a python function to step, the backend should wrap the function in a python base image automatically.","title":"Design"},{"location":"couler-tekton-design/#control-flow","text":"","title":"Control Flow"},{"location":"couler-tekton-design/#whencond-if_op-else_op","text":"In the latest v0.16 version of Tekton, the condition field is deprecated and Tekton uses whenExpression instead. Here is an example: tasks : - name : echo-file-exists when : - input : \"$(tasks.check-file.results.exists)\" operator : in values : [ \"yes\" ] taskRef : name : echo-file-exists A whenExpression is made of Input , Operator and Values . The Input can be static inputs or variables (Parameters or Results in Tekton). The Operator can be either in or notin . As we can see, there is no else_op in Tekton, so we need to convert the else in the Tekton backend, which is pretty simple since there is only in and notin operator, we can simply change the operator in the \"else\" task like: tasks : - name : echo-file-not-exists when : - input : \"$(tasks.check-file.results.exists)\" operator : notin values : [ \"yes\" ] taskRef : name : echo-file-not-exists","title":"when(cond, if_op, else_op)"},{"location":"couler-tekton-design/#while_loopcond-func-args-kwargs","text":"There is no native support for loops in Tekton for now. See this issue for more information.","title":"while_loop(cond, func, *args, **kwargs)"},{"location":"couler-tekton-design/#couler-utilities","text":"The get_status , get_logs remains the same as the Argo design. submit(config=workflow_config(schedule=\"* * * * 1\")) : There is no native support for cron in Tekton, see this issue for more information. The community recommend cronjob in kubernetes, see example here. For couler, we can wrap the cron in the workflow and let backend to handle the work like create CronJob in Kubernetes. delete_workflow(workflow_name) : To be discussed: should we delete the tasks when delete the pipeline? list_workflow_records(workflow_name) : After submitting the workflow(create PipelineRun in Tekton), there will be numbers of workflow records. A list function is necessary for users to show all the records.","title":"Couler Utilities:"},{"location":"couler-tekton-design/#other-tekton-utilities","text":"Finally : Finally is a unique feature of Tekton. With finally, the Final tasks are guaranteed to be executed in parallel after all PipelineTasks under tasks have completed regardless of success or error. For example: spec : tasks : - name : tests taskRef : Name : integration-test finally : - name : cleanup-test taskRef : Name : cleanup In Couler, we can have run_finally(finally_def) function which is only available when the backend is Tekton.","title":"Other Tekton Utilities"},{"location":"examples/","text":"Examples \u00b6 Hello World! \u00b6 Let's start by running a very simple workflow template to echo \"hello world\" using the docker/whalesay container image from DockerHub. You can run this directly from your shell with a simple docker command: $ docker run docker/whalesay cowsay \"hello world\" _____________ < hello world > ------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ With Couler, we can run the same container on a Kubernetes cluster using an Argo Workflows as the workflow engine. import couler.argo as couler from couler.argo_submitter import ArgoSubmitter couler . run_container ( image = \"docker/whalesay\" , command = [ \"cowsay\" ], args = [ \"hello world\" ] ) submitter = ArgoSubmitter () result = couler . run ( submitter = submitter ) Coin Flip \u00b6 This example combines the use of a Python function result, along with conditionals, to take a dynamic path in the workflow. In this example, depending on the result of the first step defined in flip_coin() , the template will either run the heads() step or the tails() step. Steps can be defined via either couler.run_script() for Python functions or couler.run_container() for containers. In addition, the conditional logic to decide whether to flip the coin in this example is defined via the combined use of couler.when() and couler.equal() . import couler.argo as couler from couler.argo_submitter import ArgoSubmitter def random_code (): import random res = \"heads\" if random . randint ( 0 , 1 ) == 0 else \"tails\" print ( res ) def flip_coin (): return couler . run_script ( image = \"python:alpine3.6\" , source = random_code ) def heads (): return couler . run_container ( image = \"alpine:3.6\" , command = [ \"sh\" , \"-c\" , 'echo \"it was heads\"' ] ) def tails (): return couler . run_container ( image = \"alpine:3.6\" , command = [ \"sh\" , \"-c\" , 'echo \"it was tails\"' ] ) result = flip_coin () couler . when ( couler . equal ( result , \"heads\" ), lambda : heads ()) couler . when ( couler . equal ( result , \"tails\" ), lambda : tails ()) submitter = ArgoSubmitter () couler . run ( submitter = submitter ) DAG \u00b6 This example demonstrates different ways to define the workflow as a directed-acyclic graph (DAG) by specifying the dependencies of each task via couler.set_dependencies() and couler.dag() . Please see the code comments for the specific shape of DAG that we've defined in linear() and diamond() . import couler.argo as couler from couler.argo_submitter import ArgoSubmitter def job ( name ): couler . run_container ( image = \"docker/whalesay:latest\" , command = [ \"cowsay\" ], args = [ name ], step_name = name , ) # A # / \\ # B C # / # D def linear (): couler . set_dependencies ( lambda : job ( name = \"A\" ), dependencies = None ) couler . set_dependencies ( lambda : job ( name = \"B\" ), dependencies = [ \"A\" ]) couler . set_dependencies ( lambda : job ( name = \"C\" ), dependencies = [ \"A\" ]) couler . set_dependencies ( lambda : job ( name = \"D\" ), dependencies = [ \"B\" ]) # A # / \\ # B C # \\ / # D def diamond (): couler . dag ( [ [ lambda : job ( name = \"A\" )], [ lambda : job ( name = \"A\" ), lambda : job ( name = \"B\" )], # A -> B [ lambda : job ( name = \"A\" ), lambda : job ( name = \"C\" )], # A -> C [ lambda : job ( name = \"B\" ), lambda : job ( name = \"D\" )], # B -> D [ lambda : job ( name = \"C\" ), lambda : job ( name = \"D\" )], # C -> D ] ) linear () submitter = ArgoSubmitter () couler . run ( submitter = submitter ) Note that the current version only works with Argo Workflows but we are actively working on the design of the unified interface that is extensible to additional workflow engines. Please stay tuned for more updates and we welcome any feedback and contributions from the community.","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#hello-world","text":"Let's start by running a very simple workflow template to echo \"hello world\" using the docker/whalesay container image from DockerHub. You can run this directly from your shell with a simple docker command: $ docker run docker/whalesay cowsay \"hello world\" _____________ < hello world > ------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ With Couler, we can run the same container on a Kubernetes cluster using an Argo Workflows as the workflow engine. import couler.argo as couler from couler.argo_submitter import ArgoSubmitter couler . run_container ( image = \"docker/whalesay\" , command = [ \"cowsay\" ], args = [ \"hello world\" ] ) submitter = ArgoSubmitter () result = couler . run ( submitter = submitter )","title":"Hello World!"},{"location":"examples/#coin-flip","text":"This example combines the use of a Python function result, along with conditionals, to take a dynamic path in the workflow. In this example, depending on the result of the first step defined in flip_coin() , the template will either run the heads() step or the tails() step. Steps can be defined via either couler.run_script() for Python functions or couler.run_container() for containers. In addition, the conditional logic to decide whether to flip the coin in this example is defined via the combined use of couler.when() and couler.equal() . import couler.argo as couler from couler.argo_submitter import ArgoSubmitter def random_code (): import random res = \"heads\" if random . randint ( 0 , 1 ) == 0 else \"tails\" print ( res ) def flip_coin (): return couler . run_script ( image = \"python:alpine3.6\" , source = random_code ) def heads (): return couler . run_container ( image = \"alpine:3.6\" , command = [ \"sh\" , \"-c\" , 'echo \"it was heads\"' ] ) def tails (): return couler . run_container ( image = \"alpine:3.6\" , command = [ \"sh\" , \"-c\" , 'echo \"it was tails\"' ] ) result = flip_coin () couler . when ( couler . equal ( result , \"heads\" ), lambda : heads ()) couler . when ( couler . equal ( result , \"tails\" ), lambda : tails ()) submitter = ArgoSubmitter () couler . run ( submitter = submitter )","title":"Coin Flip"},{"location":"examples/#dag","text":"This example demonstrates different ways to define the workflow as a directed-acyclic graph (DAG) by specifying the dependencies of each task via couler.set_dependencies() and couler.dag() . Please see the code comments for the specific shape of DAG that we've defined in linear() and diamond() . import couler.argo as couler from couler.argo_submitter import ArgoSubmitter def job ( name ): couler . run_container ( image = \"docker/whalesay:latest\" , command = [ \"cowsay\" ], args = [ name ], step_name = name , ) # A # / \\ # B C # / # D def linear (): couler . set_dependencies ( lambda : job ( name = \"A\" ), dependencies = None ) couler . set_dependencies ( lambda : job ( name = \"B\" ), dependencies = [ \"A\" ]) couler . set_dependencies ( lambda : job ( name = \"C\" ), dependencies = [ \"A\" ]) couler . set_dependencies ( lambda : job ( name = \"D\" ), dependencies = [ \"B\" ]) # A # / \\ # B C # \\ / # D def diamond (): couler . dag ( [ [ lambda : job ( name = \"A\" )], [ lambda : job ( name = \"A\" ), lambda : job ( name = \"B\" )], # A -> B [ lambda : job ( name = \"A\" ), lambda : job ( name = \"C\" )], # A -> C [ lambda : job ( name = \"B\" ), lambda : job ( name = \"D\" )], # B -> D [ lambda : job ( name = \"C\" ), lambda : job ( name = \"D\" )], # C -> D ] ) linear () submitter = ArgoSubmitter () couler . run ( submitter = submitter ) Note that the current version only works with Argo Workflows but we are actively working on the design of the unified interface that is extensible to additional workflow engines. Please stay tuned for more updates and we welcome any feedback and contributions from the community.","title":"DAG"},{"location":"getting-started/","text":"Installation \u00b6 Couler currently only supports Argo Workflows. Please see instructions here to install Argo Workflows on your Kubernetes cluster. Install Python 3.6+ Install Couler Python SDK via the following pip command: pip install git+https://github.com/couler-proj/couler Alternatively, you can clone this repository and then run the following to install: python setup.py install After installing Couler, run the hello world example to submit your first workflow: import couler.argo as couler from couler.argo_submitter import ArgoSubmitter couler . run_container ( image = \"docker/whalesay\" , command = [ \"cowsay\" ], args = [ \"hello world\" ] ) submitter = ArgoSubmitter () result = couler . run ( submitter = submitter ) Once the workflow is successfully submitted, the following logs will be shown: INFO:root:Found local kubernetes config. Initialized with kube_config. INFO:root:Checking workflow name/generatedName runpy- INFO:root:Submitting workflow to Argo INFO:root:Workflow runpy-ddc2m has been submitted in \"default\" namespace!","title":"Getting Started"},{"location":"getting-started/#installation","text":"Couler currently only supports Argo Workflows. Please see instructions here to install Argo Workflows on your Kubernetes cluster. Install Python 3.6+ Install Couler Python SDK via the following pip command: pip install git+https://github.com/couler-proj/couler Alternatively, you can clone this repository and then run the following to install: python setup.py install After installing Couler, run the hello world example to submit your first workflow: import couler.argo as couler from couler.argo_submitter import ArgoSubmitter couler . run_container ( image = \"docker/whalesay\" , command = [ \"cowsay\" ], args = [ \"hello world\" ] ) submitter = ArgoSubmitter () result = couler . run ( submitter = submitter ) Once the workflow is successfully submitted, the following logs will be shown: INFO:root:Found local kubernetes config. Initialized with kube_config. INFO:root:Checking workflow name/generatedName runpy- INFO:root:Submitting workflow to Argo INFO:root:Workflow runpy-ddc2m has been submitted in \"default\" namespace!","title":"Installation"}]}